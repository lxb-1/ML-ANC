{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metaaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 制作数据集\n",
    "\n",
    "When you make a metaaf dataset, you write a vanilla pytorch dataset. The dataset should not use jax and must return a dictionary with the keys \"signals\" and \"metadata\". We enforce this format since the \"signals\" are automatically segmented and buffered. In this example, we make a simple system identification dataset which returns signals but not metadata.\n",
    "\n",
    "这里通过Pytorch的`dataset`方法构造metaaf的数据集。\n",
    "\n",
    "**注意**：数据集不能使用`jax`且必须返回一个包含`signals`和`metadata`的字典。为了实现字典数据返回格式，`signals`将会被自动分段和缓冲。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import N\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 构造pytorch数据集\n",
    "class SystemIDDataset(Dataset):\n",
    "    def __init__(self, total_size=1024, N=4096, sys_order=32):\n",
    "        self.N = N\n",
    "        self.sys_order = sys_order\n",
    "        self.total_size = total_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 系统构造，其实不同的系统就是设计不同的滤波器系数矩阵w\n",
    "        w = np.random.normal(size=self.sys_order) / self.sys_order\n",
    "\n",
    "        # 系统输入信号u，即滤波器的输入\n",
    "        u = np.random.normal(size=self.N)\n",
    "\n",
    "        # 输出：系统输出信号d，即期望的目标响应(target or desired response)\n",
    "        # 通过np.convole方法返回线性离散1D卷积序列\n",
    "        d = np.convolve(w, u)[: self.N]\n",
    "\n",
    "        # meta自适应滤波数据集返回一个包含如下内容的字典：\n",
    "        # （1）\"signals\"：该信号会被自动添加到缓冲器\n",
    "        # （2）\"metadata\"：由用户自行管理\n",
    "        return {\n",
    "            \"signals\": {\n",
    "                \"u\": u[:, None],    # 时间X通道\n",
    "                \"d\": d[:, None],\n",
    "            },\n",
    "            \"metadata\": {},\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 制作滤波器\n",
    "\n",
    "滤波器是用来适应输入信号的。Metaaf为各种常见的STFT处理管道提供了封装好的代码：`Overlap-Save`、`Overlap-Add`和`Weighted Overlap-Add`。另外，通过`buffering`框架，我们也可以构建自己的滤波器。\n",
    "\n",
    "所有的滤波器都是在`haiku`库中编写的。当使用封装好的代码，滤波器必须返回一个包含`out`键的字典，它是后续STFT处理需要的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaaf.filter import OverlapSave\n",
    "\n",
    "# 滤波器继承于overlap save modules\n",
    "class SystemID(OverlapSave, hk.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 选择分析窗：\n",
    "        # 这里的self.window_size是OverlapSave中的类变量，由于这里构造的滤波器\n",
    "        # 继承于OverlapSave,所以可以直接定义self.window_size\n",
    "        self.analysis_window = jnp.ones(self.window_size)\n",
    "\n",
    "    # 由于我们使用的是OLS基类，x和y是stft域的输入\n",
    "    # 因此，The filter msut take the same inputs provided in its _fwd function.\n",
    "    def __ols_call__(self, u, d, metadata):\n",
    "        # Collect a buffer size anti-aliased filter\n",
    "        # 使用get_filter方法构造滤波器\n",
    "        w = self.get_filter(name=\"w\")\n",
    "\n",
    "        # 系统估计响应（extimated response）y和系统误差e\n",
    "        # this is n_frames x n_freq x channels or 1 x F x 1 here\n",
    "        y = w * u\n",
    "        e = y - d\n",
    "\n",
    "        return {\n",
    "            \"out\": y[0],\n",
    "            \"u\": u,\n",
    "            \"d\": d,\n",
    "            \"e\": e,\n",
    "            \"loss\": jnp.vdot(e, e).real / (e.size), # The MSE of the prediction\n",
    "        }\n",
    "\n",
    "# 为此下面提供一个封装的包，以实现通过Haiku包将对象转换为函数。\n",
    "# 封装的函数必须从数据集中获取相同命名作为输入\n",
    "def _SystemID_fwd(u, d, metadata=None, int_data=None, **kwargs):\n",
    "    gen_filter = SystemID(**kwargs)\n",
    "    return gen_filter(u=u, d=d)\n",
    "\n",
    "# 我们还需要定义滤波器的损失\n",
    "def filter_loss(out, data_samples, metadata):\n",
    "    return out[\"loss\"]\n",
    "\n",
    "# 同时我们还需要定义meta损失\n",
    "def meta_loss(losses, outputs, data_samples, metadata, outer_leanable):\n",
    "    EPS = 1e-9\n",
    "    return jnp.log(jnp.mean(jnp.abs(outputs - data_samples[\"d\"]) ** 2) + EPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化Meta训练\n",
    "\n",
    "Next, we setup all filter keyword arguments. These will be used by the OLS baseclass to correctly buffer inputs and run the online STFT processing. These can also be done via argparse, since all metaaf modules have argparse utilities. Here, we use dictionaries for simplicity.\n",
    "\n",
    "**设置所有滤波器的关键字参数：**\n",
    "\n",
    "这些参数将被用于OLS基类输入的准确缓存以及在线STFT的处理。由于所有`MetaAF`模块都具有`argparse`工具包，因此，这些操作是通过`argparse`完成的。为了简单起见，这里使用字典形式的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaaf.data import NumpyLoader\n",
    "\n",
    "# 所有滤波器的初始化参数，这些参数将被用于STFT的处理参数\n",
    "filter_kwargs = {\n",
    "    \"n_frames\": 1,\n",
    "    \"n_in_chan\": 1,\n",
    "    \"n_out_chan\": 1,\n",
    "    \"window_size\": 64,\n",
    "    \"hop_size\": 32,\n",
    "    \"pad_size\": 0,\n",
    "    \"is_real\": True,\n",
    "}\n",
    "\n",
    "# 优化器参数\n",
    "optimizer_kwargs = {\n",
    "    \"h_size\": 16,\n",
    "    \"n_layers\": 1,\n",
    "    \"lam_1\": 1e-2,\n",
    "    \"input_transform\": \"log1p\",\n",
    "}\n",
    "\n",
    "# 使用封装好的NumpyLoader类包，初始化训练需要的dataloader数据\n",
    "train_loader = NumpyLoader(SystemIDDataset(total_size=1024), batch_size=16)\n",
    "val_loader = NumpyLoader(SystemIDDataset(total_size=1024), batch_size=32)\n",
    "test_loader = NumpyLoader(SystemIDDataset(total_size=1024), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a metaaf system. This system manages the training and will later provide inference utilities. We need to pass it the forward functions, losses, keyword arguments as well as the dataloaders. We'll set some optimizer options. For more advanced functionality, we can write our own forward passes, overridde other options, and even pass in training callbacks. These could do things like save checkpoints, log outputs, and more.\n",
    "\n",
    "现在，**创建一个`MetaAF`系统：**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a663d525a294a75b868378a51d8db00740236484748f17e7fef628dae424325b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
